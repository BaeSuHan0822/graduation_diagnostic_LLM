import os
from langchain_chroma import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

current_dir = os.path.dirname(__file__)
persist_directory = os.path.join(current_dir,"chroma_db")

with open(os.path.join(current_dir,"student_db"),"r",encoding = "utf-8") as f :
    f.read()

print("âœ… ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°")
embedding_function = HuggingFaceEmbeddings(
    model_name = "intfloat/multilingual-e5-large",
    model_kwargs = {"device" : "cpu"},
    encode_kwargs = {"normalize_embeddings" : True}
)
print("âœ… ì»´í“¨í„°ê³µí•™ê³¼ Vector DB ë¶ˆëŸ¬ì˜¤ê¸° !")
computer_db = Chroma(
    persist_directory = persist_directory,
    embedding_function = embedding_function,
    collection_name = "computer_science"
)
print("âœ… ì„±ê³µ !")
print("âœ… êµì–‘ Vector DB ë¶ˆëŸ¬ì˜¤ê¸° !")
humanitas_db = Chroma(
    persist_directory = persist_directory,
    embedding_function = embedding_function,
    collection_name = "humanitas"
)
print("âœ… ì„±ê³µ")

user_query = "ì»´í“¨í„°ê³µí•™ê³¼ì˜ ì¡¸ì—…ì „ê³µí•™ì "
search_query = f"query: {user_query}"

computer_result = computer_db.similarity_search_with_score(search_query,k = 4)
humanitas_result = humanitas_db.similarity_search_with_score(search_query,k =2)

result = sorted(computer_result + humanitas_result,key = lambda x: x[1])

text = ""
for index,(document,score) in enumerate(result) :
    text += f"Document {index+1}"
    text += document.page_content

llm = ChatOllama(
    model="gemma2:2b",
    temperture=0.1
)

template = """
ë‹¹ì‹ ì€ ê¼¼ê¼¼í•œ ëŒ€í•™êµ í•™ì‚¬ í–‰ì • ë„ìš°ë¯¸ AIì…ë‹ˆë‹¤.
ë°˜ë“œì‹œ ì•„ë˜ [ì°¸ê³  ë¬¸ì„œ] ì¤‘ì—ì„œ ì°¾ì•„ì„œ ë‹µë³€í•˜ê³  ëª¨ë¥´ëŠ” ê²ƒì€ í™•ì‹¤í•˜ê²Œ ëª¨ë¥¸ë‹¤ê³  í•˜ì„¸ìš”.

ë‹µë³€í•  ë•ŒëŠ” ë‹¨ìˆœíˆ ì´ í•™ì ë§Œ ë§í•˜ì§€ ë§ê³ , ì•„ë˜ í•­ëª©ì„ í¬í•¨í•˜ì—¬ ìì„¸íˆ ì„¤ëª…í•˜ì„¸ìš”:
- ì´ ì¡¸ì—… ì´ìˆ˜ í•™ì 
- ì „ê³µ ì´ìˆ˜ í•™ì  (ê¸°ì´ˆ, í•„ìˆ˜, ì„ íƒ ë“± ì„¸ë¶€ ë‚´ì—­ í¬í•¨)
- ê¸°íƒ€ ì¤‘ìš” ìš”ê±´

[ì°¸ê³  ë¬¸ì„œ]
{context}

[ì§ˆë¬¸]
{question}

ë‹µë³€:
"""

prompt = ChatPromptTemplate.from_template(template)

rag_chain = (
    {"context": lambda x: text, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

print("ğŸ¤– Gemma2 AI ë‹µë³€:")

# ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ (íƒ€ì ì¹˜ë“¯ ë‚˜ì˜´)
for chunk in rag_chain.stream(user_query):
    print(chunk, end="", flush=True)

print("\n" + "="*50)